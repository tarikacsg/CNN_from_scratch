{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## LAYERS"
      ],
      "metadata": {
        "id": "fmSFdrFukm6U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_boston\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "lr = 10 ** -6"
      ],
      "metadata": {
        "id": "24WzBm27kXrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Flatten Layer"
      ],
      "metadata": {
        "id": "fLfXll5K53s1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class flatten:\n",
        "  def forward_pass(self, input):\n",
        "    self.input_shape = input.shape\n",
        "    self.output_shape = (np.prod(input.shape),)\n",
        "    return input.reshape(self.output_shape)\n",
        "\n",
        "  def backward_pass(self, d_out):\n",
        "    return d_out.reshape(self.input_shape)"
      ],
      "metadata": {
        "id": "KzUhD3bjenxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Matrix Multiplication Layer / Dense Layer"
      ],
      "metadata": {
        "id": "0PBKqfLf6FOJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# matrix multiplication XW\n",
        "\n",
        "class matrix_multiplication_layer:\n",
        "  def __init__(self, neurons_count, features_count):\n",
        "    self. X = np.array([])\n",
        "    self.W = np.array([[0 for j in range(neurons_count)] for i in range(features_count)])\n",
        "\n",
        "  def forward_pass(self, X):\n",
        "    self.X = X\n",
        "    # N = XW\n",
        "    N = np.dot(self.X, self.W)\n",
        "    return N\n",
        "\n",
        "  def backward_pass(self, dL_dN):\n",
        "    #dL_dW\n",
        "    dL_dW = np.dot(self.X.T, dL_dN)\n",
        "    self.W = self.W - (lr * dL_dW)\n",
        "\n",
        "    #dL_dX\n",
        "    dL_dX = np.dot(dL_dN, self.W.T)\n",
        "    return dL_dX"
      ],
      "metadata": {
        "id": "0nT0fhIpkXuV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bias Addition Layer"
      ],
      "metadata": {
        "id": "aV3JzhQPs0C5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# bias addition layer\n",
        "\n",
        "class bias_addition_layer:\n",
        "  def __init__(self, neurons_count):\n",
        "    self.N = np.array([])\n",
        "    self.B = np.array([0 for i in range(neurons_count)])\n",
        "\n",
        "  def forward_pass(self, N):\n",
        "    self.N = N\n",
        "    Z = self.N + self.B\n",
        "    return Z\n",
        "\n",
        "  def backward_pass(self, dL_dZ):\n",
        "    #dL_dB\n",
        "    dL_dB = dL_dZ.sum(axis = 0) # column wise summation\n",
        "    self.B = self.B - (lr * dL_dB)\n",
        "\n",
        "    #dL_dN\n",
        "    dL_dN = dL_dZ\n",
        "    return dL_dN"
      ],
      "metadata": {
        "id": "2tIGSbYP3zJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Softmax Layer"
      ],
      "metadata": {
        "id": "X9YGh_ju6MpV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# softmax layer\n",
        "\n",
        "class softmax_layer:\n",
        "  def __init__(self):\n",
        "    self.Z = np.array([])\n",
        "    self.softmax = np.array([])\n",
        "\n",
        "  def forward_pass(self, Z):\n",
        "    self.Z = Z\n",
        "    self.softmax = np.exp(Z)\n",
        "    row_sum_of_exponentials = np.array([[i] for i in self.softmax.sum(axis = 1)])\n",
        "    self.softmax = self.softmax / row_sum_of_exponentials\n",
        "    return self.softmax\n",
        "\n",
        "  def backward_pass(self, dL_dSoftmax):\n",
        "    dL_dZ = []\n",
        "    for i in range(len(dL_dSoftmax)):\n",
        "        cols = len(self.Z[0])\n",
        "        A = np.zeros([cols, cols])\n",
        "        S = self.softmax\n",
        "\n",
        "        for j in range(cols):\n",
        "            for k in range(cols):\n",
        "                if j == k:\n",
        "                    A[j][k] = S[i][j]*(1-S[i][k])\n",
        "                else:\n",
        "                    A[j][k] = -S[i][j]*S[i][k]\n",
        "                    \n",
        "        dL_dZ.append(np.dot(dL_dSoftmax[i], A))\n",
        "    return np.array(dL_dZ)"
      ],
      "metadata": {
        "id": "omR-U6j0kebM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sigmoid Layer"
      ],
      "metadata": {
        "id": "bYbzulcE6UJ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sigmoid layer\n",
        "\n",
        "class sigmoid_layer:\n",
        "  def __init__(self):\n",
        "    self.Z = np.array([]) # Z = N + B\n",
        "    self.sigmoid = np.array([])\n",
        "\n",
        "  def forward_pass(self, Z):\n",
        "    self.Z = Z\n",
        "    self.sigmoid = np.exp(-1 * Z)\n",
        "    self.sigmoid = 1 / (1 + self.sigmoid)\n",
        "    return self.sigmoid\n",
        "\n",
        "  def backward_pass(self, dL_dSigmoid):\n",
        "    #Z = N + B\n",
        "    dL_dZ = self.sigmoid * (1 - self.sigmoid) * dL_dSigmoid\n",
        "    return dL_dZ"
      ],
      "metadata": {
        "id": "0MEBHH2ZkefO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mean Squared Loss Layer"
      ],
      "metadata": {
        "id": "iFHGRPbT6XEI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#mean squared loss\n",
        "\n",
        "class mse_layer:\n",
        "  def __init__(self):\n",
        "    self.P = np.array([]) # predicted values\n",
        "    self.Y = np.array([]) # actual values\n",
        "\n",
        "  def forward_pass(self, P, Y):\n",
        "    self.P = P\n",
        "    self.Y = Y\n",
        "    mse = np.dot((self.P - self.Y).T, self.P - self.Y)\n",
        "    return mse\n",
        "\n",
        "  def backward_pass(self):\n",
        "    dL_dP = self.P - self.Y\n",
        "    return dL_dP"
      ],
      "metadata": {
        "id": "GoZnrB7YkxBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cross Entropy Loss Layer"
      ],
      "metadata": {
        "id": "jKuJ2uxi6ZxF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cross entropy loss layer\n",
        "\n",
        "class cross_entropy_loss_layer:\n",
        "  def __init__(self):\n",
        "    self.P = np.array([]) #predictions\n",
        "    self.Y = np.array([]) #actual labels\n",
        "\n",
        "  def forward_pass(self, P, Y):\n",
        "    self.P = P\n",
        "    self.Y = Y\n",
        "    # finding loss for each row and then adding the lossed of all the rows\n",
        "    cross_entropy_loss = (-1 * Y * np.log(P)).sum(axis = 1).sum(axis = 0)\n",
        "    return np.array([[cross_entropy_loss]])\n",
        "\n",
        "  def backward_pass(self):\n",
        "    #dL_dP\n",
        "    dL_dP = -self.Y / self.P\n",
        "    return dL_dP"
      ],
      "metadata": {
        "id": "hr2S2zhYkepG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tanh Activation Layer"
      ],
      "metadata": {
        "id": "jRenNGbX6dV1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RIg5eaGe-6UC"
      },
      "outputs": [],
      "source": [
        "#tanh layer\n",
        "\n",
        "class tanh_layer:\n",
        "    def __init__(self):\n",
        "        self.Z = np.array([])\n",
        "\n",
        "    def forward_pass(self,Z):\n",
        "        self.Z = Z\n",
        "        return np.tanh(Z)\n",
        "\n",
        "\n",
        "    def backward_pass(self, outGradient):\n",
        "        exp_z = np.exp(self.Z)\n",
        "        exp_neg_z = np.exp(-self.Z)\n",
        "        return (4 / (exp_z + exp_neg_z) **2) * outGradient"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhPDE7Kc-fAL"
      },
      "source": [
        "\n",
        "Implement the convolution layer for 1 channel input and (n >= 1) channel output. Implement both forward and backward passes. Implement the flatten operation\n",
        "https://towardsdatascience.com/forward-and-backward-propagation-in-convolutional-neural-networks-64365925fdfa"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Convolution Layer (generalized), to create a layer with 1 channel input, simply pass C=1 during object creation."
      ],
      "metadata": {
        "id": "6-z7NvRo67Rs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 0.001\n",
        "class convolution_layer:\n",
        "  #H : height inputs, W: width inputs, HH:height filter, WW: width filter\n",
        "  def __init__(self, H, W, F, C, HH, WW):\n",
        "    self.N = 1 # num of input images\n",
        "    self.H = H # height of input image\n",
        "    self.W = W # width of input image\n",
        "    self.F = F #is same as num of output channels / number of filters\n",
        "    self.C = C # number of input channels\n",
        "    self.HH = HH # height of kernel\n",
        "    self.WW = WW #width of kernel\n",
        "    # self.x = X.reshape(self.N, self.C, self.H, self.W)\n",
        "    self.w = np.random.randn(F, C, HH, WW)\n",
        "    self.b = np.random.randn(F,)\n",
        "    self.stride = 1\n",
        "    self.pad = 1\n",
        "    self.x = np.array([])\n",
        "\n",
        "  def forward_pass(self, x):\n",
        "    x = x.reshape(self.N, self.C, self.H, self.W)\n",
        "    self.x = x\n",
        "    H_output = int(1 + (self.H + 2 * self.pad - self.HH)/self.stride)\n",
        "    W_output = int(1 + (self.W + 2 * self.pad - self.WW)/self.stride)\n",
        "    output = np.zeros((self.N, self.F, H_output, W_output))\n",
        "\n",
        "    if self.pad != 0:\n",
        "      padded_x = np.pad(x, [(0,), (0,), (self.pad,), (self.pad,)], \"constant\")\n",
        "    else:\n",
        "      padded_x = x.copy()\n",
        "\n",
        "    for n in range(self.N):\n",
        "      for f in range(self.F):\n",
        "        for i in range(H_output):\n",
        "          for j in range(W_output):\n",
        "            output[n, f, i, j] = np.sum( padded_x[n, :, i*self.stride:i*self.stride+self.HH, j*self.stride : j*self.stride + self.WW] * self.w[f] ) + self.b[f]\n",
        "    return output\n",
        "\n",
        "  def backward_pass(self, derivative_out):\n",
        "    _, _, height_out, weight_out = derivative_out.shape  # For output feature maps\n",
        "\n",
        "    # Preparing gradients for output\n",
        "    dx = np.zeros_like(self.x)\n",
        "    dw = np.zeros_like(self.w)\n",
        "    db = np.zeros_like(self.b)\n",
        "    x_padded = np.pad(self.x, ((0, 0), (0, 0), (self.pad, self.pad), (self.pad, self.pad)), mode='constant', constant_values=0)\n",
        "    dx_padded = np.pad(dx, ((0, 0), (0, 0), (self.pad, self.pad), (self.pad, self.pad)), mode='constant', constant_values=0)\n",
        "\n",
        "    for n in range(self.N):\n",
        "        for f in range(self.F):\n",
        "            for i in range(0, self.H, self.stride):\n",
        "                for j in range(0, self.W, self.stride):\n",
        "                    dx_padded[n, :, i:i+self.HH, j:j+self.WW] += self.w[f, :, :, :] * derivative_out[n, f, i, j]\n",
        "                    dw[f, :, :, :] += x_padded[n, :, i:i+self.HH, j:j+self.WW] * derivative_out[n, f, i, j]\n",
        "                    db[f] += derivative_out[n, f, i, j]\n",
        "\n",
        "    dx = dx_padded[:, :, 1:-1, 1:-1]\n",
        "    #weights and biases updation\n",
        "    self.w = self.w - (lr * dw)\n",
        "    self.b = self.b - (lr * db)\n",
        "    return dx"
      ],
      "metadata": {
        "id": "LNmNYspUOb5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9aFNFS9-q5W"
      },
      "source": [
        "Train this CNN on mnist dataset. Layer 1: Convolution layer with 16 out-\n",
        "put channels+flatten+tanh activation. Layer 2: 10 output neuron with linear\n",
        "activation. Softmax cross entropy loss"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convolution Layer --> Flatten Layer --> Tanh Activation --> Matrix Multiplication / Dense Layer --> Bias Addition --> Softmax Activation --> Cross Entropy Loss Layer"
      ],
      "metadata": {
        "id": "QaaJOPTL7P9t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loading dataset\n",
        "from sklearn.datasets import load_digits\n",
        "X, Y_mnist = load_digits(return_X_y=True) #(training data, label)\n",
        "print(X.shape)\n",
        "num_of_features = len(X[0])\n",
        "num_of_labels = 10\n",
        "print(num_of_features)\n",
        "# one hot encoding on true labels\n",
        "Y = np.zeros((len(X), num_of_labels))\n",
        "for i in range(len(X)):\n",
        "  Y[i][Y_mnist[i]] = 1\n",
        "print(\"Y =\", Y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDuk8nfvUdcz",
        "outputId": "094171d5-d6c4-4116-a738-6c242b7e0292"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1797, 64)\n",
            "64\n",
            "Y = (1797, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def sgd(X, Y, num_of_iterations, model)\n",
        "def sgd_q6(X, Y, num_of_iterations):\n",
        "  loss_values = []\n",
        "\n",
        "  #defining the model\n",
        "  layer1 = convolution_layer(8, 8, 16, 1, 3, 3)\n",
        "  layer2 = flatten()\n",
        "  layer3 = tanh_layer()\n",
        "  layer4 = matrix_multiplication_layer(10, 64*16)\n",
        "  layer5 = bias_addition_layer(10)\n",
        "  layer6 = softmax_layer()\n",
        "  layer7 = cross_entropy_loss_layer()\n",
        "\n",
        "  #forward pass\n",
        "  def model_forward_pass(curr_training_input, curr_training_label):\n",
        "    convo_output = layer1.forward_pass(curr_training_input)\n",
        "    flatten_output = layer2.forward_pass(convo_output)\n",
        "    tanh_output = layer3.forward_pass(flatten_output)\n",
        "\n",
        "    tanh_output = tanh_output.reshape(1, tanh_output.shape[0])\n",
        "\n",
        "    N = layer4.forward_pass(tanh_output)\n",
        "    B = layer5.forward_pass(N)\n",
        "    P = layer6.forward_pass(B)  \n",
        "    L = layer7.forward_pass(P, curr_training_label)\n",
        "    return P, L\n",
        "\n",
        "  def model_backward_pass():\n",
        "    dL_dP = layer7.backward_pass()\n",
        "    dL_dB = layer6.backward_pass(dL_dP)\n",
        "    dL_dN = layer5.backward_pass(dL_dB)\n",
        "    dL_dT = layer4.backward_pass(dL_dN)\n",
        "    dL_dF = layer3.backward_pass(dL_dT)\n",
        "    dL_dC = layer2.backward_pass(dL_dF)\n",
        "    dL_dX = layer1.backward_pass(dL_dC)\n",
        "    \n",
        "  # training the model\n",
        "  for iter in range(num_of_iterations):\n",
        "    print(\"iter =\", iter)\n",
        "    for j in range(len(X)): # taking each sample one by one\n",
        "      curr_training_input = np.array([X[j]])\n",
        "      curr_training_label = np.array([Y[j]])\n",
        "\n",
        "      _, loss = model_forward_pass(curr_training_input, curr_training_label)\n",
        "      model_backward_pass()\n",
        "    \n",
        "    \n",
        "  #calculating the overall loss after updation of weights\n",
        "  for j in range(len(X)): # taking each sample one by one\n",
        "    curr_training_input = np.array([X[j]])\n",
        "    curr_training_label = np.array([Y[j]])\n",
        "\n",
        "    _, loss = model_forward_pass(curr_training_input, curr_training_label)\n",
        "    loss_values.append(loss[0][0])\n",
        "  print(\"LOSS VALUES =\")\n",
        "  print(loss_values[:5])\n",
        "  print(loss_values[-6:-1])\n",
        "\n",
        "  print(\"Training done.\")  "
      ],
      "metadata": {
        "id": "0BmAzUZfRoE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sgd_q6(X,Y,2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-JfREjkrSNXU",
        "outputId": "90cb795e-0409-4309-aef1-97be574763d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter = 0\n",
            "iter = 1\n",
            "LOSS VALUES =\n",
            "[0.005572199435324734, 0.005554576308231158, 0.2674621561841253, 0.014307564291114407, 0.017306181611098245]\n",
            "[0.01334367980728656, 0.027961862761544786, 0.0078003480787037964, 0.02631689970462639, 0.07024985350835393]\n",
            "Training done.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}